```yaml
layout: single
title:  "Profiling of different implementation of attention modules"
date:   2024-08-01
author_profile: true
comments: true
tags: [LLM, Attention, Profile, Pytorch, Flash-attn, Huggingface, Xformers]
```

Placeholder for the blog logging the results of profiling vllm



HUGE Overhead when the number of requests in the queue is large


