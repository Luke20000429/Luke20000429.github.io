---
layout: single
title:  "Understand CUDA Unified Memory"
date:   2024-03-31
author_profile: true
comments: true
tags: [CUDA]
---

## Basic

Unified virtual memory (UVM) is introduced in CUDA since Pascal Architecture, it is designed to unified the memory of **hosts (CPU)** and **devices (GPU)** into the same address space, so that a piece data can be accessed by any host code and kernel code. 

<!-- The most widely known and applied feature of unified memory is automatic data transfer via page faults. Advanced techniques like residence-advising and prefetching further boost its performance. 

However, until today (03/31/2024), another feature of UVM, paged memory, is still not considered and applied as far as I can tell. In this post, I will introduce my understanding and advanced usage of that feature. -->

The basic usage of UVM is already introduced in [Unified Memory for CUDA Beginners](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/) and [Maximizing Unified Memory Performance in CUDA](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/). This post is to log my experiments with CUDA unified memory and some innovative and interesting application of UVM in large language model (LLM).

The name, "unified virtual memory", actually indicates two major features, "unified" and "virtual". We will exploit both of them in this study.

## Using CuPy library

As the goal of this study is to apply UVM in LLM, I use [CuPy]([https://cupy.dev/](https://cupy.dev/)) for fast demonstration and profiling. CUDA version might be added later.

UVM is aliased as managed memory in CUDA APIs. There are two ways of allocating managed memory with cupy.

```python
import cupy as cp 

num_float32 = 1000
arr_dtype = cp.float32
# Method 1: using managed memory allocate API, similar to C++
memory = cp.cuda.malloc_managed(num_float32 * 4) # unit: byte
x_cp = cp.ndarray(num_float32, dtype=arr_dtype, memptr=memory)

# Method 2: set memory allocator to managed_allocator, more python style
cp.cuda.set_allocator(cp.cuda.malloc_managed)
y_cp = cp.ndarray(num_float32, dtype=arr_dtype)
```

To use it in [PyTorch](https://pytorch.org/) for machine learning (ML) purpose, users only need to <u>wrap it up with tensor interface</u>,

```python
import torch
# in-place transform the array to a tensor
x_tensor = torch.as_tensor(x_cp, device='cuda')
# in-place transform a part of the array to a tensor
subx_tensor = torch.as_tensor(x_cp[:100], device='cpu')
print(x_tensor.size(), subx_tensor.size())
```

---

**<mark>NOTE</mark>**: Specify the `device` of tensor doesn't effect where the data locates, but to avoid meta data conflicts.

---

Output:

```bash
torch.Size([1000]) torch.Size([100])
```

You can verify they are on the same physical memory by 

```python
x_cp[0] = 100
x_tensor[1] = 11
subx_tensor[2] = 22
print(x_cp[:10])
```

Output:

```bash
[100.  11.  22.   0.   0.   0.   0.   0.   0.   0.]
```

which means any modification is done on the same piece of data.

## Unified address

Data allocated in unified memory can be accessed seamlessly on devices and host, and users don't need to call methods like `.cuda()` or `.cpu()` to deal with the memory transfer. Instead, the UVM engine will handle the memory transfer between devices and host triggered by page faults

![](/assets/images/blogs/2024-03-29-cuda_uvm/1530143595229.PNG)

## Virtual paged memory

<!-- You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-title.MARKUP`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `MARKUP` is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:

{% highlight ruby %}
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
{% endhighlight %}

Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ -->
