---
layout: single
title:  "Understand CUDA Unified Memory"
date:   2024-03-31
author_profile: true
comments: true
tags: [CUDA]
---

## Basic

Unified virtual memory (UVM) is introduced in CUDA since Pascal Architecture, it is designed to unified the memory of **hosts (CPU)** and **devices (GPU)** into the same address space, so that a piece of data can be accessed by any host code and kernel code. 

The basic usage of UVM is already introduced in [Unified Memory for CUDA Beginners](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/) and [Maximizing Unified Memory Performance in CUDA](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/). This post is to log my experiments with CUDA unified memory and some innovative and interesting application of UVM in large language model (LLM).

The name, "unified virtual memory", actually indicates two major features, "[unified](#unified-address)" and "[virtual](#virtual-paged-memory)". We will exploit both of them in this study. 

## Using CuPy library

As the goal of this study is to apply UVM in LLM, I use [CuPy]([https://cupy.dev/](https://cupy.dev/)) for fast demonstration and profiling. CUDA version might be added later.

UVM is aliased as `cudaMemManaged` in CUDA APIs. There are two ways of allocating managed memory with cupy.

```python
import cupy as cp 

num_float32 = 1000
arr_dtype = cp.float32
# Method 1: using managed memory allocate API, similar to C++
memory = cp.cuda.malloc_managed(num_float32 * 4) # unit: byte
x_cp = cp.ndarray(num_float32, dtype=arr_dtype, memptr=memory)

# Method 2: set memory allocator to managed_allocator, more python style
cp.cuda.set_allocator(cp.cuda.malloc_managed)
y_cp = cp.ndarray(num_float32, dtype=arr_dtype)
```

To use it in [PyTorch](https://pytorch.org/) for machine learning (ML) purpose, users only need to <u>wrap it up with tensor interface</u>,

```python
import torch
# in-place transform the array to a tensor
x_tensor = torch.as_tensor(x_cp, device='cuda')
# in-place transform a part of the array to a tensor
subx_tensor = torch.as_tensor(x_cp[:100], device='cpu')
print(x_tensor.size(), subx_tensor.size())
```

---

**<mark>NOTE</mark>**: Specify the `device` of tensor doesn't effect where the data locates, but to avoid meta data conflicts.

---

Output:

```bash
torch.Size([1000]) torch.Size([100])
```

You can verify they are on the same physical memory by 

```python
x_cp[0] = 100
x_tensor[1] = 11
subx_tensor[2] = 22
print(x_cp[:10])
```

Output:

```bash
[100.  11.  22.   0.   0.   0.   0.   0.   0.   0.]
```

which means any modification is done on the same piece of data.

## Unified address

Data allocated in unified memory can be accessed seamlessly on devices and host, and users don't need to call methods like `.cuda()` or `.cpu()` to deal with the memory transfer. Instead, the UVM engine will handle the memory transfer between devices and host triggered by page faults. TODO: add a figure of nsys. As is shown in [using cupy library](#using-cupy-library), the data can be accessed either as CPU memory or CUDA memory.

Besides, UVM supports oversubscription, which means the size of managed memory can goes over the size of GPU memory, and the oversized part will be stored on host. Once a GPU kernel code tries to r/w to that address, the UVM engine swap the data to device. 

## Virtual paged memory
